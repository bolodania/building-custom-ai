{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"./images/btp-banner.gif\" alt=\"BTP A&C\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation with SAP HANA Cloud Vector Engine\n",
    "\n",
    "In this demo, we will explore how to enhance the capabilities of Large Language Models (LLMs) with **SAP HANA Cloud vector engine**. You will learn how to embed unstructured and semi-structured data using AI models from **SAP Generative AI Hub**, and store the vector embeddings in **SAP HANA Cloud**. Additionally, you will query vector embeddings, and forward the relevant results to a LLM to generate an augmented answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØLearning Objectives\n",
    "By the end of this demo, you will be able to:\n",
    "- Implement a full RAG pipeline using Python, LangChain, and Generative AI Hub SDK.\n",
    "- Generate embeddings for document chunks using Generative AI Hub SDK.\n",
    "- Retrieve the most relevant content based on semantic similarity using SAP HANA Cloud similarity search.\n",
    "- Augment user prompts with retrieved context and invoke LLMs to generate more accurate, grounded answers.\n",
    "- Design and use prompt templates to enhance the quality of generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö®Requirements\n",
    "\n",
    "Before starting the Jupyter Notebook steps, ensure the following: \n",
    "- Enable the additional feature **Natural Language Processing (NLP)** in your SAP HANA Cloud database \n",
    "- Deploy AI models in SAP AI Launchpad:\n",
    "  - Large Language Model (LLM) for chat/completion: **`anthropic--claude-4.5-sonnet`** \n",
    "  - Embedding model for vector representations: **`text-embedding-3-large`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìùAbout the Data\n",
    "\n",
    "The data set is a product catalog of IT accessory products. Here are the main attributes and their descriptions based on the sample data:\n",
    "\n",
    "|Field          |Description            |\n",
    "----------------|-----------------------\n",
    "|**PRODUCT_ID**| A unique identifier for each product.|\n",
    "|**PRODUCT_NAME**| The name of the product, which typically includes the brand and the model.|\n",
    "|**CATEGORY**| The general category of the product, which is \"IT Accessories\" for all entries sampled.|\n",
    "|**DESCRIPTION**| A detailed description of the product, highlighting key features and specifications.|\n",
    "|**UNIT_PRICE**| The price of the product in Euros.|\n",
    "|**UNIT_MEASURE**| The unit of measure for the product, typically \"Each\" indicating pricing per item.|\n",
    "|**SUPPLIER_ID**| A unique identifier for the supplier of the product.|\n",
    "|**SUPPLIER_NAME**| The name of the supplier.|\n",
    "|**LEAD_TIME_DAYS**| The number of days it takes from order to delivery.|\n",
    "|**MIN_ORDER**| The minimum order quantity required.|\n",
    "|**CURRENCY**| The currency of the transaction, which is \"EURO\" for all entries.|\n",
    "|**SUPPLIER_COUNTRY**| The country where the supplier is located, which is \"Germany\" for all sampled entries.|\n",
    "|**SUPPLIER_ADDRESS**| The physical address of the supplier.|\n",
    "|**AVAILABILITY_DAYS**| The number of days the product is available for delivery.|\n",
    "|**SUPPLIER_CITY**| The city where the supplier is located.|\n",
    "|**STOCK_QUANTITY**| The quantity of the product currently in stock.|\n",
    "|**MANUFACTURER**| The company that manufactured the product.|\n",
    "|**CITY_LAT**| Geographical coordinates of the city (latitude)|\n",
    "|**CITY_LONG**| Geographical coordinates of the city (longitude).|\n",
    "|**RATING**| A rating for the product, which are on a scale from 1 to 5.|\n",
    "\n",
    "</br>\n",
    "\n",
    "This dataset is structured to support various business operations such as inventory management, order processing, and logistics planning, providing a comprehensive view of product offerings, supplier details, and stock levels. Each entry is highly detailed, suggesting the dataset could be used for analytical purposes, such as optimizing supply chain operations or analyzing sales and marketing strategies.\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation using generative AI Hub and SAP HANA Vector Engine\n",
    "\n",
    "### Hands-on Retrieval Augmented Generation (RAG) workflow \n",
    "\n",
    "The Retrieval Augmented Generation use case process consists of steps to be completed as seen in the graphic below. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "> ![title](./images/rag_full.png)\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "#### Indexing Process\n",
    "1. Business documents that should be used for answering user questions are fed into the model. The contents of the files are split into smaller chunks.\n",
    "    >\"Chunking\" (and sometimes called \"LLM chunking\") refers to dividing a large text corpus into smaller, manageable pieces or segments. Each recursive chunking part acts as a standalone unit of information that can be individually indexed and retrieved. \n",
    "2. Embedding functions are used to create embeddings from the file/document chunks.\n",
    "    >Embeddings refer to dense, continuous vectors representing text in a high-dimensional space. These vectors serve as coordinates in a semantic space, capturing the relationships and meanings between words.\n",
    "3. The embeddings are then stored as vectors in the SAP HANA Cloud Database.\n",
    "#### Retrieval Process\n",
    "1. A query or prompt is submitted.\n",
    "2. The query embedded into a vector form.\n",
    "3. The query vector is compared to the values stored as vectors in SAP HANA Cloud via a similarity/semantic search.\n",
    "4. The most appropriate and relevant results are identified.\n",
    "5. And forwarded, along with the original query, to a large language model.\n",
    "6. The LLM uses the results of the HANA vector search to augment its own searching capabilities, and the final answer is returned to the user.\n",
    "\n",
    "### Implementing RAG Embeddings\n",
    "\n",
    "- Prepare the documentation for the product catalog in CSV format with each row representing a product.\n",
    "- Connect to the HANA vector storage instance and create a table to store the documentation data.\n",
    "- Populate the table with data and create a REAL_VECTOR column to store embeddings.\n",
    "- Use the Generative AI Hub SDK to define a function to generate embeddings for prompts and perform similarity search using the embeddings.\n",
    "\n",
    "### Enhancing Query Responses\n",
    "\n",
    "- Define a prompt template to provide context to queries.\n",
    "- Modify the function to query the LLM (Large Language Model) based on the prompt template.\n",
    "- Test the model's response using specific queries related to the node library, ensuring it provides contextually relevant responses based on embeddings.\n",
    "  \n",
    "> Retrieval augmented generation optimizes the output of large language models by applying more context to prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and configuration\n",
    "\n",
    "The following Python modules are to be installed during this hands-on introduction. \n",
    "\n",
    "#### **sap-ai-sdk-gen**\n",
    "\n",
    "With this SAP python SDK you can leverage the power of generative Models like chatGPT available in SAP's generative AI Hub.\n",
    "\n",
    "For more information, please see https://pypi.org/project/sap-ai-sdk-gen/\n",
    "\n",
    "#### **hdbcli**\n",
    "\n",
    "The Python Database API Specification v2.0 (PEP 249) defines a set of methods that provides a consistent database interface independent of the actual database being used. The Python extension module for SAP HANA implements PEP 249. Once you install the module, you can access and change the information in SAP HANA databases from Python.\n",
    "\n",
    "For more information, please see https://pypi.org/project/hdbcli/\n",
    "\n",
    "#### **langchain-hana**\n",
    "\n",
    "Integrates LangChain with SAP HANA Cloud to make use of vector search, knowledge graph, and further in-database capabilities as part of LLM-driven applications.\n",
    "\n",
    "For more information, please see https://pypi.org/project/langchain-hana/\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "#### Install Python packages\n",
    "\n",
    "Run the following package installations. **pip** is the package installer for Python. You can use pip to install packages from the Python Package Index and other indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"sap-ai-sdk-gen[all]\" --break-system-packages -U\n",
    "%pip install hdbcli --break-system-packages -U\n",
    "%pip install langchain-hana --break-system-packages -U\n",
    "\n",
    "# kernel restart required!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart Python kernel\n",
    "\n",
    "The Python kernel needs to be restarted before continuing. \n",
    "\n",
    "> ![title](./images/config_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure SAP Generative AI Hub credentials\n",
    "\n",
    "Execute the configuration module below to enable access to SAP Generative AI foundation models. This configuration is automatically done by running configuration module in the code block.\n",
    "\n",
    "You could also set up the same by running a terminal command: **aicore configure**\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "> Please ensure that the Python kernel was restarted!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative AI Config\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'env_config.json')) as f:\n",
    "    aicore_config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM model\n",
    "LLM is initialized with a model **anthropic--claude-4.5-sonnet**. This is used for generating responses or interacting in a chat-like environment.\n",
    "\n",
    "> **IMPORTANT!** here you are connecting to the **anthropic--claude-4.5-sonnet** model that was deployed in SAP AI Core.\n",
    "<!-- We can compare how the output produced by RAG is different from the output when we directly pass the prompt to the model. If we directly pass the prompt to the model without RAG, this will be the output. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set llm\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "from gen_ai_hub.proxy.gen_ai_hub_proxy import GenAIHubProxyClient\n",
    "# harmonized init helper\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "# Set up the AICoreV2Client\n",
    "ai_core_client = AICoreV2Client(base_url=aicore_config['AICORE_BASE_URL'],\n",
    "                            auth_url=aicore_config['AICORE_AUTH_URL'],\n",
    "                            client_id=aicore_config['AICORE_CLIENT_ID'],\n",
    "                            client_secret=aicore_config['AICORE_CLIENT_SECRET'],\n",
    "                            resource_group=aicore_config['AICORE_RESOURCE_GROUP'])\n",
    "# Set up the GenAIHubProxyClient\n",
    "proxy_client = GenAIHubProxyClient(ai_core_client = ai_core_client)\n",
    "print(\"‚úÖAI Core Client connection is established successfully!\")\n",
    "\n",
    "# Set up the LLM model (here we are using Anthropic Claude 4.5 Sonnet as an example but you can choose any available model from here https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/README_sphynx.html#supported-models)\n",
    "llm = init_llm('anthropic--claude-4.5-sonnet', proxy_client=proxy_client, max_tokens=800, temperature=0.3, top_p=None)\n",
    "print(\"‚úÖLLM model connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask LLM without context\n",
    "\n",
    "After completing the configuration we are ready to ask the first question directly to LLM (anthropic--claude-4.5-sonnet) without any business product context to find us products with a rating of 4 and more. The response is arbitrary and does not relate to our product data. \n",
    "\n",
    "</br>\n",
    "\n",
    "> **Note** We can solve this problem by following the next steps in implementing RAG Embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Define a simple prompt template for querying the LLM to get a keyboard suggestion based on the information from the Internet (for now).\n",
    "TEMPLATE = \"\"\"Advise the user based on the information available to you from general sources. Question: {question}\"\"\"\n",
    "\n",
    "# Create a HumanMessagePromptTemplate using the defined template.\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(TEMPLATE)\n",
    "# Create a ChatPromptTemplate using the human message prompt defined above.\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "question = \"Return one keyboard suggestion (brand + model) with rating ‚â•4. Format as: 'Keyboard: <name>'\"\n",
    "\n",
    "# Format the prompt with the input question\n",
    "prompt_text = chat_prompt.format_prompt(question=question).to_string()\n",
    "\n",
    "# Invoke the LLM with the formatted prompt and get the response\n",
    "llm_response = llm.invoke(prompt_text)\n",
    "\n",
    "# Display the LLM response in a readable format\n",
    "display(Markdown(llm_response.content.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Retrieval Augmented Generation (RAG)\n",
    "\n",
    "### Prepare the documentation for the product catalog in CSV format with each row representing a product\n",
    "\n",
    "This code snippet demonstrates how to load and process text data from a CSV file using the `CSVLoader` from the `langchain_community.document_loaders` library.\n",
    "\n",
    "This process is useful for handling large text data, making it more manageable or suitable for further processing, analysis, or input into machine learning models, especially when dealing with limitations on input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "FILE_PRODUCTS = \"data/product_catalog.csv\"\n",
    "\n",
    "# Process CSV data file\n",
    "loader = CSVLoader(\n",
    "    file_path=FILE_PRODUCTS,\n",
    "    source_column=\"PRODUCT_ID\",\n",
    "    metadata_columns=[\n",
    "        \"SUPPLIER_ID\",\n",
    "        \"CATEGORY\",\n",
    "        \"SUPPLIER_COUNTRY\",\n",
    "        \"SUPPLIER_CITY\",\n",
    "        \"MANUFACTURER\",\n",
    "    ],\n",
    "    content_columns=[\n",
    "        \"PRODUCT_NAME\",\n",
    "        \"DESCRIPTION\",\n",
    "        \"UNIT_PRICE\",\n",
    "        \"LEAD_TIME_DAYS\",\n",
    "        \"STOCK_QUANTITY\",\n",
    "        \"RATING\",\n",
    "        \"MIN_ORDER\",\n",
    "        \"CATEGORY\",\n",
    "        \"SUPPLIER_NAME\",\n",
    "        \"SUPPLIER_COUNTRY\",\n",
    "        \"SUPPLIER_CITY\",\n",
    "        \"SUPPLIER_ADDRESS\",\n",
    "        \"STATUS\",\n",
    "        \"CURRENCY\",\n",
    "        \"MANUFACTURER\",\n",
    "        \"CITY_LAT\",\n",
    "        \"CITY_LONG\",\n",
    "    ],\n",
    "    csv_args={\"delimiter\": \";\", \"quotechar\": '\"'},\n",
    "    encoding=\"utf-8-sig\",\n",
    ")\n",
    "\n",
    "# Process data\n",
    "text_documents = loader.load()\n",
    "print(f\"üìÑ Loaded {len(text_documents)} documents\")\n",
    "\n",
    "for chunks in text_documents:\n",
    "    print(chunks.metadata)\n",
    "    print(chunks.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At this point we have implemented the first RAG step - generated text chunks from source data\n",
    "> \n",
    "> ![rag_indexing_1](./images/rag_indexing_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAP HANA Cloud vector engine\n",
    "\n",
    "Storing vector embeddings within the same database is a strategic move that aligns seamlessly with SAP's commitment to providing a unified platform. This integration eliminates the hurdles posed by data silos, offering a holistic approach to data management. In SAP HANA Cloud, the storage of vector embeddings is seamlessly integrated into the platform's existing structure, allowing users to store them in a designated table. Developers can perform SQL-like queries effortlessly. \n",
    "\n",
    "This means you can execute joins, apply filters, and perform selects by combining vector embeddings with various data types, including transactional, spatial, graph, and JSON data, all within the same SQL environment. The Vector Engine ensures a user-friendly experience, eliminating the need for extensive learning or the adoption of new querying methodologies. Essentially, working with vector embeddings in SAP HANA Cloud is as straightforward as crafting queries in a standard SQL database, offering familiarity and ease of use for developers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to the HANA vector storage instance\n",
    "\n",
    "The provided Python script imports database connection modules and initiates a connection to a SAP HANA Cloud instance using the `dbapi` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HC Vector Engine\n",
    "import os\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Load HANA Cloud connection details\n",
    "with open(os.path.join(os.getcwd(), 'env_cloud.json')) as f:\n",
    "    hana_env_c = json.load(f)\n",
    "\n",
    "# Establish a connection to the HANA Cloud database\n",
    "connection = dbapi.connect( \n",
    "    address=hana_env_c['url'],\n",
    "    port=hana_env_c['port'], \n",
    "    user=hana_env_c['user'], \n",
    "    password=hana_env_c['pwd']   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and store embeddings in SAP HANA Cloud\n",
    "\n",
    "We will be using two types of Embedding Models to generate the embeddings: **text-embedding-3-large** from OpenAI and **SAP_GXY.20250407** from SAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenAI embedding model\n",
    "Embeddings are vector representations of text data that incorporate the semantic meaning of the text. Define the embeddings object that generates embeddings from text data using the **text-embedding-3-large** model. This function will be used to generate embeddings from the user's prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonized init helper\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "open_ai_embedding_model = init_embedding_model('text-embedding-3-large', proxy_client=proxy_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the OpenAI embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input string\n",
    "text = \"SAP Generative AI Hub is awesome!\"\n",
    "# Generate the embedding\n",
    "embedding = open_ai_embedding_model.embed_query(text)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the table with data and create a REAL_VECTOR column to store embeddings\n",
    "\n",
    "Create a LangChain VectorStore interface for the HANA database and specify the table (collection) to use for accessing the vector embeddings. Embeddings are vector representations of text data that incorporate the semantic meaning of the text.\n",
    "\n",
    "The `langchain_hana` library, specifically the `HanaDB` class, from the LangChain community, enables interaction with SAP HANA Cloud's vector storage capabilities. It provides tools to manage and query vector embeddings stored in SAP HANA Cloud, making it easier to implement workflows like Retrieval Augmented Generation (RAG) using SAP HANA's advanced database features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_hana import HanaDB\n",
    "# Create a LangChain VectorStore interface for the HANA database and specify the table (collection) to use for accessing the vector embeddings\n",
    "db_openai_table = HanaDB(\n",
    "    embedding=open_ai_embedding_model, \n",
    "    connection=connection, \n",
    "    table_name=\"PRODUCTS_IT_ACCESSORY_OPENAI_\"+ hana_env_c['user'],\n",
    "    content_column=\"VEC_TEXT\", # the original text description of the product details\n",
    "    metadata_column=\"VEC_META\", # metadata associated with the product details\n",
    "    vector_column=\"VEC_VECTOR\" # the vector representation of each product \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete already existing documents from the table\n",
    "db_openai_table.delete(filter={})\n",
    "\n",
    "# add the loaded document chunks\n",
    "db_openai_table.add_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify product embeddings in SAP HANA Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table to verify embeddings\n",
    "cursor = connection.cursor()\n",
    "sql = f'SELECT VEC_TEXT, TO_NVARCHAR(VEC_VECTOR) FROM \"{db_openai_table.table_name}\"'\n",
    "\n",
    "cursor.execute(sql)\n",
    "vectors = cursor.fetchall()\n",
    "\n",
    "for vector in vectors:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At this point we have implemented the Indexing Process part of RAG\n",
    "> \n",
    "> ![rag_indexing_2](./images/rag_indexing_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a Vector Search to Find Relevant Products  \n",
    "\n",
    "## Vector Search Using OpenAI's Embedding Model (*text-embedding-3-large*)\n",
    "\n",
    "In this step, we use the **text-embedding-3-large** model to convert a natural language query into a vector representation and retrieve the most relevant records from a database using **vector similarity search**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Query \n",
    "question = \"Suggest a keyboard with a rating 4 or more\"\n",
    "# Using an embedding model (text-embedding-3-large), we transform the text query into a numerical vector\n",
    "# Then, we perform a similarity search in the HANA database to find the most relevant product information based on the vector representation of the query. \n",
    "# The results are returned with their similarity scores, which indicate how closely they match the query.\n",
    "results = db_openai_table.similarity_search_with_score(query=question, k=10)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search Using SAP's Embedding Model (*SAP_GXY.20250407*)\n",
    "\n",
    "You will:\n",
    "\n",
    "1. **Create a table** in SAP HANA to store text data and generate vector embeddings using **SAP_GXY.20250407**.  \n",
    "2. **Insert product descriptions** into the table, allowing automatic embedding generation.  \n",
    "3. **Perform a vector search** to retrieve the most relevant products based on semantic similarity.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a table with automatic vector embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_hana import HanaInternalEmbeddings\n",
    "\n",
    "# HANA-internal embedding\n",
    "internal_emb = HanaInternalEmbeddings(internal_embedding_model_id=\"SAP_GXY.20250407\")\n",
    "\n",
    "db_internal_emb_table = HanaDB(\n",
    "    embedding=internal_emb, \n",
    "    connection=connection, \n",
    "    table_name=\"PRODUCTS_IT_ACCESSORY_SAP_\"+ hana_env_c['user'],\n",
    "    content_column=\"VEC_TEXT\", # the original text description of the product details\n",
    "    metadata_column=\"VEC_META\", # metadata associated with the product details\n",
    "    vector_column=\"VEC_VECTOR\" # the vector representation of each product \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting data in SAP HANA Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete already existing documents from the table\n",
    "db_internal_emb_table.delete(filter={})\n",
    "\n",
    "# add the loaded document chunks\n",
    "db_internal_emb_table.add_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the table to verify stored embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table to verify embeddings\n",
    "cursor = connection.cursor()\n",
    "sql = f'SELECT VEC_TEXT, TO_NVARCHAR(VEC_VECTOR) FROM \"{db_internal_emb_table.table_name}\"'\n",
    "\n",
    "cursor.execute(sql)\n",
    "vectors = cursor.fetchall()\n",
    "\n",
    "for vector in vectors:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a vector search using SAP's embedding model\n",
    "\n",
    "- Using an internal SAP HANA embedding model `SAP_GXY.20250407` we transform the text query into a numerical vector\n",
    "- Then, we perform a similarity search in the HANA database to find the most relevant product information based on the vector representation of the query. \n",
    "- The results are returned with their similarity scores, which indicate how closely they match the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Suggest a keyboard with a rating 4 or more\"\n",
    "\n",
    "results = db_internal_emb_table.similarity_search_with_score(query=question, k=10)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At this point we have successfully implemented the first step of Retrieval Process and enabled semantic vector similarity search to **retrieve relevant results from SAP HANA Cloud** based on user question.\n",
    "> \n",
    "> ![rag_retrieval_1](./images/rag_retrieval_1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Query Responses by Passing Context and Prompt to LLM\n",
    "\n",
    "### Define a prompt template to provide context to queries\n",
    "\n",
    "Define a prompt template to provide context to our prompts. Thus, when passed to the model, the template will add the necessary context to the prompt so that more accurate results are generated.\n",
    "\n",
    "> The created template for the prompt contains two variables - **`context`** and **`query`**. These variables will be replaced with the context and question in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Define the prompt template for product recommendation using retrieved context from HANA database.\n",
    "# This prompt guides the LLM to provide accurate and relevant product recommendations based solely on the information retrieved from the database, without making any assumptions or fabricating details. The prompt emphasizes the importance of adhering to the provided context and clearly outlines the expected response format and behavior in various scenarios (e.g., when no products match the criteria or when information is incomplete).\n",
    "PRODUCT_RAG_PROMPT = \"\"\"\n",
    "    You are a product recommendation assistant.\n",
    "\n",
    "    Your job is to help users find and understand products using ONLY the information provided in the retrieved context.\n",
    "\n",
    "    You must follow these rules strictly:\n",
    "\n",
    "    --------------------------------------------------\n",
    "    1. Use only retrieved context\n",
    "    --------------------------------------------------\n",
    "    - Base your answer only on the provided context.\n",
    "    - Never invent product names, specifications, ratings, or prices.\n",
    "    - If required information is missing, say you don‚Äôt know.\n",
    "\n",
    "    --------------------------------------------------\n",
    "    2. Understand user intent\n",
    "    --------------------------------------------------\n",
    "    The user may ask to:\n",
    "    - recommend products\n",
    "    - filter products by criteria (rating, price, category, brand, features)\n",
    "    - compare products\n",
    "    - explain product features\n",
    "    - summarize options\n",
    "    - find best match for a need\n",
    "\n",
    "    Interpret the request and use the context to respond appropriately.\n",
    "\n",
    "    --------------------------------------------------\n",
    "    3. When recommending products\n",
    "    --------------------------------------------------\n",
    "    If matching products exist:\n",
    "    - return only products that meet the criteria\n",
    "    - clearly list product name and relevant attributes\n",
    "    - explain briefly why each product matches\n",
    "\n",
    "    If no products match:\n",
    "    Respond exactly:\n",
    "    \"I could not find any products that match your criteria.\"\n",
    "\n",
    "    --------------------------------------------------\n",
    "    4. When information is incomplete\n",
    "    --------------------------------------------------\n",
    "    If the context does not contain enough information:\n",
    "    Respond exactly:\n",
    "    \"I don‚Äôt have enough information to answer that.\"\n",
    "\n",
    "    --------------------------------------------------\n",
    "    5. Do not expose system details\n",
    "    --------------------------------------------------\n",
    "    Never mention:\n",
    "    - embeddings\n",
    "    - vector search\n",
    "    - retrieval\n",
    "    - metadata\n",
    "    - internal processing\n",
    "\n",
    "    --------------------------------------------------\n",
    "    6. Response style\n",
    "    --------------------------------------------------\n",
    "    - Be clear and concise\n",
    "    - Use structured lists when helpful\n",
    "    - Be factual and neutral\n",
    "    - Do not speculate\n",
    "\n",
    "    --------------------------------------------------\n",
    "\n",
    "    User question:\n",
    "    {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "# Create a HumanMessagePromptTemplate using the defined prompt template. \n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(PRODUCT_RAG_PROMPT)\n",
    "\n",
    "# Create a ChatPromptTemplate using the human message prompt defined above. \n",
    "# This template will be used to format the input query and the retrieved context from the HANA database when invoking the LLM for generating product recommendations based on the retrieved information.\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Query and Contextual Product Recommendation\n",
    "\n",
    "- This code block demonstrates how to use a vector similarity search to retrieve the most relevant product information from SAP HANA Cloud based on a user query with specific criteria (e.g., rating, availability, delivery time).\n",
    "- The retrieved context is then formatted and passed to a Large Language Model (LLM), which generates product recommendations strictly grounded in the actual data, ensuring accurate and context-aware responses\n",
    "- The approach combines semantic search and generative AI to deliver precise, criteria-based product suggestions, avoiding fabricated or irrelevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more complex query that requires the LLM to utilize the retrieved context effectively to provide accurate and relevant product recommendations based on specific criteria such as rating, availability, and delivery time.\n",
    "question = \"Suggest keyboards with a rating 4 or more. Make sure that the suggested products are available for an order and can be delivered in 10 days or less.\"\n",
    "\n",
    "# Using an embedding model (text-embedding-3-large), we transform the text query into a numerical vector\n",
    "# Then, we perform a similarity search in the HANA database to find the most relevant product information based on the vector representation of the query.\n",
    "# The results are returned with their similarity scores, which indicate how closely they match the query.\n",
    "vector_search_results = db_openai_table.similarity_search_with_score(\n",
    "    query=question, k=10\n",
    ")\n",
    "\n",
    "# We extract the relevant context from the search results to provide it as input to the LLM for generating a response that is grounded in the retrieved information from the HANA database.\n",
    "context = \"\\n\\n\".join(doc.page_content for doc, _ in results)\n",
    "\n",
    "# Finally, we format the prompt with the user query and the retrieved context to create the final input that will be sent to the LLM for generating product recommendations based on the information retrieved from the HANA database.\n",
    "prompt_text = chat_prompt.format_prompt(query=question, context=context).to_string()\n",
    "\n",
    "# We invoke the LLM with the formatted prompt to generate a response that provides product recommendations based on the retrieved context from the HANA database, adhering to the guidelines specified in the prompt template.\n",
    "llm_response = llm.invoke(prompt_text)\n",
    "\n",
    "print(\"üí¨LLM Response:\")\n",
    "display(Markdown(llm_response.content.strip()))\n",
    "\n",
    "print(\"\\n\\nüíØThis is the end of the Jupiter Notebook. Thank you for your attention!\")\n",
    "print(\"üì∏ Please take a screenshot!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At this point we have successfully implemented the final step of Retrieval Process and enabled generation of the **contextually relevant answer** based on user's query.\n",
    ">\n",
    "> ![rag_full](./images/rag_full.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Database Connection\n",
    "Ensure the database connection is closed when the notebook is no longer in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cursor:\n",
    "    cursor.close()\n",
    "if connection:\n",
    "    connection.close()\n",
    "print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
